{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 지난주 개념 복습!\n",
        "아래 마크다운을 풀고, 지난주 예습과제를 통해 공부한 Vision Transformer의 특징과 다른 모델들과의 차이점을 간략히 설명해주세요."
      ],
      "metadata": {
        "id": "KTJ_ipcIRCw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vision Transformer (ViT)\n",
        "는 이미지를 고정 크기 패치로 나눠 토큰처럼 보고, 전역(Self-Attention) 으로 관계를 학습한다.\n",
        "\n",
        "Patch Embedding(선형투영) → [CLS] 토큰 + 절대 위치임베딩 → 여러 층의 Transformer Encoder의 구조로 이루어 지며,\n",
        "\n",
        "전역 문맥에 강하고 단순한 구조지만, 데이터와 정규화에 민감하며 복잡도로 해상도가 커지면 계산량이 급증하기 때문에, 주로 대규모 데이터/프리트레인을 사용 가능하고 단순한 백본 원할 때(분류) 사용된다.\n",
        "\n",
        "- Swin Transformer\n",
        " 는  계층적 피처 피라미드와 지역 윈도우 기반(Self-Attention), Shifted Window로 창을 번갈아 이동시켜 전역 상호작용 확보한다.\n",
        "\n",
        "따라서 CNN처럼 지역성,계층성 유도, 다양한 해상도 입력 처리에 강하기 때문에,고해상도 입력, 검출/세그먼테이션처럼 다단계 피처가 중요한 경우에 주로 사용된다.\n",
        "\n",
        "\n",
        "- CvT (Convolutional vision Transformer) 는 토큰을 만들고 다운샘플링할 때 합성곱을 결합한 컨볼루션 기반 토큰 임베딩/프로젝션을 사용한다.\n",
        "\n",
        "여러 단계를 거치면서 채널 수는 점차 늘어나고 해상도는 점차 낮아지는 피라미드 구조를 형성하며, 합성곱과 결합된 어텐션으로 공간 정보와 채널 정보를 함께 혼합·처리한다.\n",
        "\n",
        "이 모델은 초기 단계부터 지역적 유도 편향과 일체형 다운샘플링을 도입해 데이터 효율성과 학습 안정성을 높이므로, 학습 데이터가 비교적 적거나 초기 수렴 안정성과 데이터 효율이 중요한 상황에서 주로 활용된다.\n",
        "\n",
        "초기에 지역적 유도편향과 번들된 다운샘플링으로 데이터 효율성과 학습 안정성이 높기 때문에 학습 데이터가 상대적으로 적거나 초기 수렴 안정성/데이터 효율이 중요한 경우에 주로 사용된다.\n",
        "\n"
      ],
      "metadata": {
        "id": "hzgR6HMqU8GL"
      }
    },
    {
      "metadata": {
        "id": "2X1nX0HO8Pzo"
      },
      "cell_type": "markdown",
      "source": [
        "## 파이토치로 구현한 Vision Transformer\n",
        "이번 복습과제에서는 카사바 잎 질병 데이터셋으로 vision transformer를 학습시키겠습니다.\n",
        "- 첨부된 드라이브에서 필요한 파일들을 다운받을 수 있습니다."
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "VvuOjOVx8Pzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01088e57-14c2-4fda-9db6-a4f6cf10a392"
      },
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version 1.7\n",
        "!pip install timm\n",
        "!pip install torch_xla"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    14  100    14    0     0     89      0 --:--:-- --:--:-- --:--:--    90\n",
            "  File \"/content/pytorch-xla-env-setup.py\", line 1\n",
            "    404: Not Found\n",
            "    ^^^\n",
            "SyntaxError: illegal target for annotation\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cpu)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (12.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.10.5)\n",
            "Requirement already satisfied: torch_xla in /usr/local/lib/python3.12/dist-packages (2.8.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from torch_xla) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_xla) (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from torch_xla) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_xla) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_xla) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_xla) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_xla) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_xla) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 불러오기"
      ],
      "metadata": {
        "id": "mkzDbeyLJiMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xZ5JSsRJ-jLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2531878-c719-476d-fe25-97e7bbbe896a"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#.zip 파일 unzip\n",
        "#본인 드라이브의 경로에 맞게 수정해주세요!\n",
        "!unzip /content/drive/MyDrive/mydata/train_images -d /content/drive/MyDrive/mydata/train_images"
      ],
      "metadata": {
        "id": "KTSPp_3DFtT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d4619a-4ab6-4e01-aaa8-476347494792"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/mydata/train_images, /content/drive/MyDrive/mydata/train_images.zip or /content/drive/MyDrive/mydata/train_images.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#본인 드라이브의 경로에 맞게 수정해주세요!\n",
        "!unzip /content/drive/MyDrive/mydata/jx_vit_base_p16_224-80ecf9dd.pth.zip -d /content/drive/MyDrive/mydata/"
      ],
      "metadata": {
        "id": "DFWWlF7qHtLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32014e09-a586-4e84-b1fa-7f9718168b4d"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/mydata/jx_vit_base_p16_224-80ecf9dd.pth.zip, /content/drive/MyDrive/mydata/jx_vit_base_p16_224-80ecf9dd.pth.zip.zip or /content/drive/MyDrive/mydata/jx_vit_base_p16_224-80ecf9dd.pth.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#파일 경로\n",
        "DATA_PATH = \"/content/drive/MyDrive/mydata/\"\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/mydata/train_images\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/mydata/test.jpg\"\n",
        "MODEL_PATH = (\n",
        "    \"/content/drive/MyDrive/mydata/jx_vit_base_p16_224-80ecf9dd.pth/jx_vit_base_p16_224-80ecf9dd.pth\"\n",
        ")"
      ],
      "metadata": {
        "id": "-RDZl6MkGG8Q"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "BASE_DIR = \"/content/drive/MyDrive/mydata/train_images\"\n",
        "disk_files = glob.glob(os.path.join(BASE_DIR, \"*.jpg\"))\n",
        "disk_names = set(os.path.basename(p) for p in disk_files)\n",
        "\n",
        "# CSV를 존재 파일로 필터링\n",
        "filtered_df = train_df[train_df[\"image_id\"].isin(disk_names)].copy().reset_index(drop=True)\n",
        "print(\"원래 CSV:\", len(train_df), \" -> 존재 파일만:\", len(filtered_df))\n",
        "\n",
        "# 경로 컬럼 만들어 두고 그걸 Dataset에서 사용\n",
        "filtered_df[\"__path__\"] = filtered_df[\"image_id\"].apply(lambda n: os.path.join(BASE_DIR, n))\n",
        "\n",
        "train_df = filtered_df  # 이후 학습은 이걸로\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZYhfa8zjdC-",
        "outputId": "3b4cad49-ad8f-4b46-d32a-c62d50571cc7"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원래 CSV: 19257  -> 존재 파일만: 5455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 라이브러리"
      ],
      "metadata": {
        "id": "Ku7W6KWzKDD4"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pXIxH6bV8Pzp"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "import timm\n",
        "\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn import model_selection, metrics"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7FVK6SER8Pzp"
      },
      "cell_type": "code",
      "source": [
        "os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
        "os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\""
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Eg-fSDgT8Pzq"
      },
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    \"\"\"\n",
        "    Seeds basic parameters for reproductibility of results\n",
        "\n",
        "    Arguments:\n",
        "        seed {int} -- Number of the seed\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "seed_everything(1001)"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 파라미터"
      ],
      "metadata": {
        "id": "FtIGnNOrKHpD"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "id": "6-dhvr7d8Pzq"
      },
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224  #ViT에 적합한 이미지 크기로 설정해주세요.\n",
        "BATCH_SIZE = 16\n",
        "LR = 2e-05\n",
        "GAMMA = 0.7\n",
        "N_EPOCHS = 1  #모델 학습을 위해서는 더 많은 에포크가 필요하지만, 파일 크기가 큰 관계로 이번 과제에서는 전 과정을 한 번만 수행하겠습니다."
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "mmTbSP278Pzr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a74f920d-caaf-4b72-cd4b-c481504a0e5c"
      },
      "cell_type": "code",
      "source": [
        "# pandas로 csv 파일을 읽은 뒤, 첫 5행을 출력하세요.\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/mydata/train.csv\")\n",
        "df.head()\n"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         image_id  label\n",
              "0  1000015157.jpg      0\n",
              "1  1000201771.jpg      3\n",
              "2   100042118.jpg      1\n",
              "3  1000723321.jpg      1\n",
              "4  1000812911.jpg      3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eee9c267-ba53-4177-962a-f95cee6339bb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000015157.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000201771.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100042118.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000723321.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000812911.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eee9c267-ba53-4177-962a-f95cee6339bb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eee9c267-ba53-4177-962a-f95cee6339bb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eee9c267-ba53-4177-962a-f95cee6339bb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1ce2aa4f-3d5b-4727-90bd-53690840a17c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1ce2aa4f-3d5b-4727-90bd-53690840a17c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1ce2aa4f-3d5b-4727-90bd-53690840a17c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 21397,\n  \"fields\": [\n    {\n      \"column\": \"image_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21397,\n        \"samples\": [\n          \"2615227158.jpg\",\n          \"1277648239.jpg\",\n          \"2305895487.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0RY7_DJ48Pzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61b30b3a-698b-416f-a931-ac153dad3c3a"
      },
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21397 entries, 0 to 21396\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   image_id  21397 non-null  object\n",
            " 1   label     21397 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 334.5+ KB\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_qkEJrdC8Pzs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "b7ad8214-851c-4288-fe20-cd2a325937cc"
      },
      "cell_type": "code",
      "source": [
        "df.label.value_counts().plot(kind=\"bar\")"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='label'>"
            ]
          },
          "metadata": {},
          "execution_count": 179
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GnBvbpb38Pzs"
      },
      "cell_type": "code",
      "source": [
        "# train과 test를 9:1로 나눠주세요.\n",
        "# random_state은 자유롭게 지정 가능합니다.\n",
        "train_df, valid_df = model_selection.train_test_split(\n",
        "    df, test_size=0.1, random_state=42, stratify=df.label.values\n",
        ")"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-dBGym2r8Pzs"
      },
      "cell_type": "code",
      "source": [
        "class CassavaDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, data_path=DATA_PATH, mode=\"train\", transforms=None):\n",
        "        super().__init__()\n",
        "        self.df_data = df.values\n",
        "        self.data_path = data_path\n",
        "        self.transforms = transforms\n",
        "        self.mode = mode\n",
        "        self.data_dir = \"train_images\" if mode == \"train\" else \"test_images\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name, label = self.df_data[index]\n",
        "        img_path = os.path.join(self.data_path, self.data_dir, img_name)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(img)\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "R4_JLjHx8Pzs"
      },
      "cell_type": "code",
      "source": [
        "#데이터 증강\n",
        "transforms_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(p=0.3),\n",
        "        transforms.RandomResizedCrop(IMG_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "transforms_valid = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ovuryvnO8Pzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c843996c-f585-4c59-c47e-e213c7107cde"
      },
      "cell_type": "code",
      "source": [
        "#사용 가능한 ViT 모델 목록\n",
        "##우리는 이중 vit_base_patch16_224를 사용합니다!\n",
        "print(\"Available Vision Transformer Models: \")\n",
        "timm.list_models(\"vit*\")"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Vision Transformer Models: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vit_7b_patch16_dinov3',\n",
              " 'vit_base_mci_224',\n",
              " 'vit_base_patch8_224',\n",
              " 'vit_base_patch14_dinov2',\n",
              " 'vit_base_patch14_reg4_dinov2',\n",
              " 'vit_base_patch16_18x2_224',\n",
              " 'vit_base_patch16_224',\n",
              " 'vit_base_patch16_224_miil',\n",
              " 'vit_base_patch16_384',\n",
              " 'vit_base_patch16_clip_224',\n",
              " 'vit_base_patch16_clip_384',\n",
              " 'vit_base_patch16_clip_quickgelu_224',\n",
              " 'vit_base_patch16_dinov3',\n",
              " 'vit_base_patch16_dinov3_qkvb',\n",
              " 'vit_base_patch16_gap_224',\n",
              " 'vit_base_patch16_plus_240',\n",
              " 'vit_base_patch16_plus_clip_240',\n",
              " 'vit_base_patch16_reg4_gap_256',\n",
              " 'vit_base_patch16_rope_224',\n",
              " 'vit_base_patch16_rope_ape_224',\n",
              " 'vit_base_patch16_rope_mixed_224',\n",
              " 'vit_base_patch16_rope_mixed_ape_224',\n",
              " 'vit_base_patch16_rope_reg1_gap_256',\n",
              " 'vit_base_patch16_rpn_224',\n",
              " 'vit_base_patch16_siglip_224',\n",
              " 'vit_base_patch16_siglip_256',\n",
              " 'vit_base_patch16_siglip_384',\n",
              " 'vit_base_patch16_siglip_512',\n",
              " 'vit_base_patch16_siglip_gap_224',\n",
              " 'vit_base_patch16_siglip_gap_256',\n",
              " 'vit_base_patch16_siglip_gap_384',\n",
              " 'vit_base_patch16_siglip_gap_512',\n",
              " 'vit_base_patch16_xp_224',\n",
              " 'vit_base_patch32_224',\n",
              " 'vit_base_patch32_384',\n",
              " 'vit_base_patch32_clip_224',\n",
              " 'vit_base_patch32_clip_256',\n",
              " 'vit_base_patch32_clip_384',\n",
              " 'vit_base_patch32_clip_448',\n",
              " 'vit_base_patch32_clip_quickgelu_224',\n",
              " 'vit_base_patch32_plus_256',\n",
              " 'vit_base_patch32_siglip_256',\n",
              " 'vit_base_patch32_siglip_gap_256',\n",
              " 'vit_base_r26_s32_224',\n",
              " 'vit_base_r50_s16_224',\n",
              " 'vit_base_r50_s16_384',\n",
              " 'vit_base_resnet26d_224',\n",
              " 'vit_base_resnet50d_224',\n",
              " 'vit_betwixt_patch16_gap_256',\n",
              " 'vit_betwixt_patch16_reg1_gap_256',\n",
              " 'vit_betwixt_patch16_reg4_gap_256',\n",
              " 'vit_betwixt_patch16_reg4_gap_384',\n",
              " 'vit_betwixt_patch16_rope_reg4_gap_256',\n",
              " 'vit_betwixt_patch32_clip_224',\n",
              " 'vit_giant_patch14_224',\n",
              " 'vit_giant_patch14_clip_224',\n",
              " 'vit_giant_patch14_dinov2',\n",
              " 'vit_giant_patch14_reg4_dinov2',\n",
              " 'vit_giant_patch16_gap_224',\n",
              " 'vit_giantopt_patch16_siglip_256',\n",
              " 'vit_giantopt_patch16_siglip_384',\n",
              " 'vit_giantopt_patch16_siglip_gap_256',\n",
              " 'vit_giantopt_patch16_siglip_gap_384',\n",
              " 'vit_gigantic_patch14_224',\n",
              " 'vit_gigantic_patch14_clip_224',\n",
              " 'vit_gigantic_patch14_clip_378',\n",
              " 'vit_gigantic_patch14_clip_quickgelu_224',\n",
              " 'vit_huge_patch14_224',\n",
              " 'vit_huge_patch14_clip_224',\n",
              " 'vit_huge_patch14_clip_336',\n",
              " 'vit_huge_patch14_clip_378',\n",
              " 'vit_huge_patch14_clip_quickgelu_224',\n",
              " 'vit_huge_patch14_clip_quickgelu_378',\n",
              " 'vit_huge_patch14_gap_224',\n",
              " 'vit_huge_patch14_xp_224',\n",
              " 'vit_huge_patch16_gap_448',\n",
              " 'vit_huge_plus_patch16_dinov3',\n",
              " 'vit_huge_plus_patch16_dinov3_qkvb',\n",
              " 'vit_intern300m_patch14_448',\n",
              " 'vit_large_patch14_224',\n",
              " 'vit_large_patch14_clip_224',\n",
              " 'vit_large_patch14_clip_336',\n",
              " 'vit_large_patch14_clip_quickgelu_224',\n",
              " 'vit_large_patch14_clip_quickgelu_336',\n",
              " 'vit_large_patch14_dinov2',\n",
              " 'vit_large_patch14_reg4_dinov2',\n",
              " 'vit_large_patch14_xp_224',\n",
              " 'vit_large_patch16_224',\n",
              " 'vit_large_patch16_384',\n",
              " 'vit_large_patch16_dinov3',\n",
              " 'vit_large_patch16_dinov3_qkvb',\n",
              " 'vit_large_patch16_rope_224',\n",
              " 'vit_large_patch16_rope_ape_224',\n",
              " 'vit_large_patch16_rope_mixed_224',\n",
              " 'vit_large_patch16_rope_mixed_ape_224',\n",
              " 'vit_large_patch16_siglip_256',\n",
              " 'vit_large_patch16_siglip_384',\n",
              " 'vit_large_patch16_siglip_512',\n",
              " 'vit_large_patch16_siglip_gap_256',\n",
              " 'vit_large_patch16_siglip_gap_384',\n",
              " 'vit_large_patch16_siglip_gap_512',\n",
              " 'vit_large_patch32_224',\n",
              " 'vit_large_patch32_384',\n",
              " 'vit_large_r50_s32_224',\n",
              " 'vit_large_r50_s32_384',\n",
              " 'vit_little_patch16_reg1_gap_256',\n",
              " 'vit_little_patch16_reg4_gap_256',\n",
              " 'vit_medium_patch16_clip_224',\n",
              " 'vit_medium_patch16_gap_240',\n",
              " 'vit_medium_patch16_gap_256',\n",
              " 'vit_medium_patch16_gap_384',\n",
              " 'vit_medium_patch16_reg1_gap_256',\n",
              " 'vit_medium_patch16_reg4_gap_256',\n",
              " 'vit_medium_patch16_rope_reg1_gap_256',\n",
              " 'vit_medium_patch32_clip_224',\n",
              " 'vit_mediumd_patch16_reg4_gap_256',\n",
              " 'vit_mediumd_patch16_reg4_gap_384',\n",
              " 'vit_mediumd_patch16_rope_reg1_gap_256',\n",
              " 'vit_pe_core_base_patch16_224',\n",
              " 'vit_pe_core_gigantic_patch14_448',\n",
              " 'vit_pe_core_large_patch14_336',\n",
              " 'vit_pe_core_small_patch16_384',\n",
              " 'vit_pe_core_tiny_patch16_384',\n",
              " 'vit_pe_lang_gigantic_patch14_448',\n",
              " 'vit_pe_lang_large_patch14_448',\n",
              " 'vit_pe_spatial_base_patch16_512',\n",
              " 'vit_pe_spatial_gigantic_patch14_448',\n",
              " 'vit_pe_spatial_large_patch14_448',\n",
              " 'vit_pe_spatial_small_patch16_512',\n",
              " 'vit_pe_spatial_tiny_patch16_512',\n",
              " 'vit_pwee_patch16_reg1_gap_256',\n",
              " 'vit_relpos_base_patch16_224',\n",
              " 'vit_relpos_base_patch16_cls_224',\n",
              " 'vit_relpos_base_patch16_clsgap_224',\n",
              " 'vit_relpos_base_patch16_plus_240',\n",
              " 'vit_relpos_base_patch16_rpn_224',\n",
              " 'vit_relpos_base_patch32_plus_rpn_256',\n",
              " 'vit_relpos_medium_patch16_224',\n",
              " 'vit_relpos_medium_patch16_cls_224',\n",
              " 'vit_relpos_medium_patch16_rpn_224',\n",
              " 'vit_relpos_small_patch16_224',\n",
              " 'vit_relpos_small_patch16_rpn_224',\n",
              " 'vit_small_patch8_224',\n",
              " 'vit_small_patch14_dinov2',\n",
              " 'vit_small_patch14_reg4_dinov2',\n",
              " 'vit_small_patch16_18x2_224',\n",
              " 'vit_small_patch16_36x1_224',\n",
              " 'vit_small_patch16_224',\n",
              " 'vit_small_patch16_384',\n",
              " 'vit_small_patch16_dinov3',\n",
              " 'vit_small_patch16_dinov3_qkvb',\n",
              " 'vit_small_patch16_rope_224',\n",
              " 'vit_small_patch16_rope_ape_224',\n",
              " 'vit_small_patch16_rope_mixed_224',\n",
              " 'vit_small_patch16_rope_mixed_ape_224',\n",
              " 'vit_small_patch32_224',\n",
              " 'vit_small_patch32_384',\n",
              " 'vit_small_plus_patch16_dinov3',\n",
              " 'vit_small_plus_patch16_dinov3_qkvb',\n",
              " 'vit_small_r26_s32_224',\n",
              " 'vit_small_r26_s32_384',\n",
              " 'vit_small_resnet26d_224',\n",
              " 'vit_small_resnet50d_s16_224',\n",
              " 'vit_so150m2_patch16_reg1_gap_256',\n",
              " 'vit_so150m2_patch16_reg1_gap_384',\n",
              " 'vit_so150m2_patch16_reg1_gap_448',\n",
              " 'vit_so150m_patch16_reg4_gap_256',\n",
              " 'vit_so150m_patch16_reg4_gap_384',\n",
              " 'vit_so150m_patch16_reg4_map_256',\n",
              " 'vit_so400m_patch14_siglip_224',\n",
              " 'vit_so400m_patch14_siglip_378',\n",
              " 'vit_so400m_patch14_siglip_384',\n",
              " 'vit_so400m_patch14_siglip_gap_224',\n",
              " 'vit_so400m_patch14_siglip_gap_378',\n",
              " 'vit_so400m_patch14_siglip_gap_384',\n",
              " 'vit_so400m_patch14_siglip_gap_448',\n",
              " 'vit_so400m_patch14_siglip_gap_896',\n",
              " 'vit_so400m_patch16_siglip_256',\n",
              " 'vit_so400m_patch16_siglip_384',\n",
              " 'vit_so400m_patch16_siglip_512',\n",
              " 'vit_so400m_patch16_siglip_gap_256',\n",
              " 'vit_so400m_patch16_siglip_gap_384',\n",
              " 'vit_so400m_patch16_siglip_gap_512',\n",
              " 'vit_srelpos_medium_patch16_224',\n",
              " 'vit_srelpos_small_patch16_224',\n",
              " 'vit_tiny_patch16_224',\n",
              " 'vit_tiny_patch16_384',\n",
              " 'vit_tiny_r_s16_p8_224',\n",
              " 'vit_tiny_r_s16_p8_384',\n",
              " 'vit_wee_patch16_reg1_gap_256',\n",
              " 'vit_xsmall_patch16_clip_224',\n",
              " 'vitamin_base_224',\n",
              " 'vitamin_large2_224',\n",
              " 'vitamin_large2_256',\n",
              " 'vitamin_large2_336',\n",
              " 'vitamin_large2_384',\n",
              " 'vitamin_large_224',\n",
              " 'vitamin_large_256',\n",
              " 'vitamin_large_336',\n",
              " 'vitamin_large_384',\n",
              " 'vitamin_small_224',\n",
              " 'vitamin_xlarge_256',\n",
              " 'vitamin_xlarge_336',\n",
              " 'vitamin_xlarge_384']"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 설계\n",
        "- 클래스 ViTBase16의 각 줄을 설명하는 주석을 달아주세요. (최소 15개)"
      ],
      "metadata": {
        "id": "UfLeB5QwLWR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTBase16(nn.Module):\n",
        "    def __init__(self, n_classes, pretrained=False):\n",
        "        # nn.Module 초기화: 부모 클래스의 생성자를 호출하여 내부 시스템(등록된 파라미터 등)을 준비\n",
        "        super(ViTBase16, self).__init__()\n",
        "\n",
        "        # timm 라이브러리로 ViT-Base (patch size 16, 224 입력) 모델을 생성 (기본은 임베딩/블록 등 백본 포함)\n",
        "        self.model = timm.create_model(\"vit_base_patch16_224\", pretrained=False)\n",
        "        # pretrained=True로 들어오면 사용자 정의 경로의 가중치를 불러와서 모델에 로드\n",
        "        if pretrained:\n",
        "            self.model.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "        # 분류 헤드 교체: 기존 head의 입력 차원(in_features)은 유지하고, 출력 차원을 n_classes로 변경\n",
        "        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 순전파 정의: timm 모델에 입력 x를 전달하여 로짓(미분류 점수)을 얻음\n",
        "        x = self.model(x)\n",
        "        # 최종 출력 반환 (손실 함수/소프트맥스는 바깥에서 적용)\n",
        "        return x\n",
        "\n",
        "    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n",
        "        # 한 에폭 동안의 평균 손실과 정확도를 누적할 변수 초기화\n",
        "        epoch_loss = 0.0\n",
        "        epoch_accuracy = 0.0\n",
        "\n",
        "        # 학습 모드로 전환: 드롭아웃/배치정규화 등 학습 동작 활성화\n",
        "        self.model.train()\n",
        "        # 배치 반복: i는 배치 인덱스, data는 입력 이미지, target은 정답 라벨\n",
        "        for i, (data, target) in enumerate(train_loader):\n",
        "            # CUDA 환경이면 텐서를 GPU로 이동 (dtype은 DataLoader/Transform에서 이미 맞춰졌다고 가정)\n",
        "            if device.type == \"cuda\":\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            # TPU(XLA) 환경이면 XLA 디바이스로 이동하며 dtype을 명시 (float32/long)\n",
        "            elif device.type == \"xla\":\n",
        "                data = data.to(device, dtype=torch.float32)\n",
        "                target = target.to(device, dtype=torch.int64)\n",
        "\n",
        "            # 이전 스텝의 그래디언트 초기화\n",
        "            optimizer.zero_grad()\n",
        "            # 모델 순전파로 로짓 계산\n",
        "            output = self.forward(data)\n",
        "            # 손실 계산(예: CrossEntropyLoss), output은 [B, n_classes], target은 [B]\n",
        "            loss = criterion(output, target)\n",
        "            # 역전파로 그래디언트 계산\n",
        "            loss.backward()\n",
        "            # 배치 정확도 계산: 예측 클래스(argmax)와 타깃 일치 비율\n",
        "            accuracy = (output.argmax(dim=1) == target).float().mean()\n",
        "\n",
        "            # 에폭 누적량 갱신(텐서 상태로 누적; 마지막에 배치 수로 평균)\n",
        "            epoch_loss += loss\n",
        "            epoch_accuracy += accuracy\n",
        "\n",
        "            # TPU(XLA)에서는 일반 optimizer.step 대신 xm.optimizer_step을 호출해야 실제 업데이트가 적용됨\n",
        "            if device.type == \"xla\":\n",
        "                xm.optimizer_step(optimizer)\n",
        "\n",
        "                # 마스터 프로세스에서만 간단한 진행 로그를 20배치마다 출력\n",
        "                if i % 20 == 0:\n",
        "                    xm.master_print(f\"\\tBATCH {i+1}/{len(train_loader)} - LOSS: {loss}\")\n",
        "\n",
        "            else:\n",
        "                # CUDA/CPU 환경: 일반적인 옵티마이저 스텝\n",
        "                optimizer.step()\n",
        "\n",
        "        # 에폭 평균 손실과 평균 정확도를 반환 (배치 수로 나눔)\n",
        "        return epoch_loss / len(train_loader), epoch_accuracy / len(train_loader)\n",
        "\n",
        "    def validate_one_epoch(self, valid_loader, criterion, device):\n",
        "        # 검증 손실과 정확도 누적 변수 초기화\n",
        "        valid_loss = 0.0\n",
        "        valid_accuracy = 0.0\n",
        "\n",
        "        # 평가 모드로 전환: 드롭아웃/배치정규화가 평가 동작(고정 통계)으로 바뀜\n",
        "        self.model.eval()\n",
        "        # 검증 데이터 배치 반복\n",
        "        for data, target in valid_loader:\n",
        "            # CUDA면 GPU로 이동\n",
        "            if device.type == \"cuda\":\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            # XLA면 XLA 디바이스로 이동 (dtype 명시)\n",
        "            elif device.type == \"xla\":\n",
        "                data = data.to(device, dtype=torch.float32)\n",
        "                target = target.to(device, dtype=torch.int64)\n",
        "\n",
        "            # 검증에서는 학습/그래디언트 계산 비활성화로 메모리/속도 최적화\n",
        "            with torch.no_grad():\n",
        "                # 순전파로 로짓 계산\n",
        "                output = self.model(data)\n",
        "                # 손실 및 정확도 계산\n",
        "                loss = criterion(output, target)\n",
        "                accuracy = (output.argmax(dim=1) == target).float().mean()\n",
        "\n",
        "                # 누적\n",
        "                valid_loss += loss\n",
        "                valid_accuracy += accuracy\n",
        "\n",
        "        # 배치 평균 손실과 정확도 반환\n",
        "        return valid_loss / len(valid_loader), valid_accuracy / len(valid_loader)\n"
      ],
      "metadata": {
        "id": "JLdlCGtoeiGh"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_tpu(\n",
        "    model, epochs, device, criterion, optimizer, train_loader, valid_loader=None\n",
        "):\n",
        "    # 최적의 검증 손실을 무한대로 초기화하여 이후 더 작은 값이 나타나면 갱신\n",
        "    valid_loss_min = np.inf\n",
        "\n",
        "    # 에폭별 기록을 저장할 리스트들 (손실/정확도: 학습/검증)\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    train_accs = []\n",
        "    valid_accs = []\n",
        "\n",
        "    # 1부터 epochs까지 반복하면서 학습 루프 수행\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # 파이썬 가비지 컬렉션 강제 호출로 메모리 파편화/누수를 완화\n",
        "        gc.collect()\n",
        "        # TPU(XLA) 전용 병렬 로더: DataLoader를 XLA 디바이스용 per-device 로더로 변환\n",
        "        para_train_loader = pl.ParallelLoader(train_loader, [device])\n",
        "\n",
        "        # TPU 멀티 프로세스 환경에서 \"마스터\" 프로세스만 로그를 출력\n",
        "        xm.master_print(f\"{'='*50}\")\n",
        "        xm.master_print(f\"EPOCH {epoch} - TRAINING...\")\n",
        "        # 한 에폭 학습 수행: 모델 내부의 train_one_epoch가 실제 학습/역전파/옵티마이저 스텝 담당\n",
        "        train_loss, train_acc = model.train_one_epoch(\n",
        "            para_train_loader.per_device_loader(device), criterion, optimizer, device\n",
        "        )\n",
        "        # 현재 에폭의 학습 손실/정확도 로그 출력\n",
        "        xm.master_print(\n",
        "            f\"\\n\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, ACCURACY: {train_acc}\\n\"\n",
        "        )\n",
        "        # 기록 리스트에 에폭 결과를 저장 (이때 텐서일 수 있으므로 그대로 보관)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        # 학습 직후 불필요 객체 수거 시도 (메모리 안정화 목적)\n",
        "        gc.collect()\n",
        "\n",
        "        # 검증 데이터로 평가를 수행할지 여부 확인 (None이면 스킵)\n",
        "        if valid_loader is not None:\n",
        "            # 검증 전에도 가비지 컬렉션 호출\n",
        "            gc.collect()\n",
        "            # 검증용 ParallelLoader 생성 (TPU/XLA 환경에서 안전한 평가를 위해 필요)\n",
        "            para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n",
        "            xm.master_print(f\"EPOCH {epoch} - VALIDATING...\")\n",
        "            # 한 에폭 검증 수행: 역전파 없이 손실/정확도 측정\n",
        "            valid_loss, valid_acc = model.validate_one_epoch(\n",
        "                para_valid_loader.per_device_loader(device), criterion, device\n",
        "            )\n",
        "            # 검증 결과 로그 출력\n",
        "            xm.master_print(f\"\\t[VALID] LOSS: {valid_loss}, ACCURACY: {valid_acc}\\n\")\n",
        "            # 기록 리스트에 검증 결과 누적\n",
        "            valid_losses.append(valid_loss)\n",
        "            valid_accs.append(valid_acc)\n",
        "            gc.collect()\n",
        "\n",
        "            # 이전까지의 최소 검증 손실보다 낮으면(성능 향상) 체크포인트 저장 알림\n",
        "            # (epoch != 1 조건은 첫 에폭에서의 형식적 메시지 출력을 방지하기 위함)\n",
        "            if valid_loss <= valid_loss_min and epoch != 1:\n",
        "                xm.master_print(\n",
        "                    \"Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...\".format(\n",
        "                        valid_loss_min, valid_loss\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            # 최소 검증 손실 갱신(다음 에폭 비교를 위해 최신값 보관)\n",
        "            valid_loss_min = valid_loss\n",
        "\n",
        "    # 최종적으로 학습 과정에서 수집한 지표들을 딕셔너리 형태로 반환\n",
        "    return {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"valid_losses\": valid_losses,\n",
        "        \"train_acc\": train_accs,\n",
        "        \"valid_acc\": valid_accs,\n",
        "    }"
      ],
      "metadata": {
        "id": "7HMQ_Pkiencf"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GV_5ivmr8Pzu"
      },
      "cell_type": "code",
      "source": [
        "model = ViTBase16(n_classes=5, pretrained=True)"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-y_dQgzz8Pzu"
      },
      "cell_type": "code",
      "source": [
        "# --- torch_xla world_size/rank 호환 유틸 ---\n",
        "try:\n",
        "    import torch_xla.runtime as xr  # PJRT (신 API)\n",
        "    def _world_size(): return xr.world_size()\n",
        "    def _rank(): return xr.global_ordinal()\n",
        "except Exception:\n",
        "    import torch_xla.core.xla_model as xm  # 구/중간 API\n",
        "    def _world_size():\n",
        "        # xla_world_size가 있으면 우선 사용, 없으면 xrt_world_size로 폴백\n",
        "        return getattr(xm, \"xla_world_size\", getattr(xm, \"xrt_world_size\"))()\n",
        "    def _rank(): return xm.get_ordinal()\n",
        "\n",
        "def _run():\n",
        "    # NOTE: 실제로 검증셋이 따로 있으면 아래 valid_dataset에서 train_df 대신 valid_df를 사용하세요.\n",
        "    train_dataset = CassavaDataset(train_df, transforms=transforms_train)\n",
        "    valid_dataset = CassavaDataset(train_df, transforms=transforms_valid)  # <- valid_df 있으면 교체\n",
        "\n",
        "    world_size = _world_size()\n",
        "    rank = _rank()\n",
        "\n",
        "    # world_size가 1이면 DistributedSampler를 생략하고 DataLoader의 shuffle을 사용\n",
        "    if world_size > 1:\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            train_dataset,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank,\n",
        "            shuffle=True,\n",
        "        )\n",
        "        valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            valid_dataset,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank,\n",
        "            shuffle=False,\n",
        "        )\n",
        "        train_sampler_arg = train_sampler\n",
        "        valid_sampler_arg = valid_sampler\n",
        "        train_shuffle = False\n",
        "    else:\n",
        "        train_sampler_arg = None\n",
        "        valid_sampler_arg = None\n",
        "        train_shuffle = True  # 단일 프로세스면 DataLoader의 shuffle 사용\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=train_sampler_arg,\n",
        "        shuffle=train_shuffle,\n",
        "        drop_last=True,\n",
        "        num_workers=8,\n",
        "    )\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        dataset=valid_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=valid_sampler_arg,\n",
        "        shuffle=False,\n",
        "        drop_last=True,\n",
        "        num_workers=8,\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()  # cross entropy loss\n",
        "    device = xm.xla_device()\n",
        "    model.to(device)\n",
        "\n",
        "    # 글로벌 배치 스케일링을 가정한 학습률 조정 (필요 없으면 LR 그대로 쓰세요)\n",
        "    lr = LR * world_size\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    xm.master_print(f\"INITIALIZING TRAINING ON {world_size} TPU CORES\")\n",
        "    start_time = datetime.now()\n",
        "    xm.master_print(f\"Start Time: {start_time}\")\n",
        "\n",
        "    logs = fit_tpu(\n",
        "        model=model,\n",
        "        epochs=N_EPOCHS,\n",
        "        device=device,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "    )\n",
        "\n",
        "    xm.master_print(f\"Execution time: {datetime.now() - start_time}\")\n",
        "\n",
        "    xm.master_print(\"Saving Model\")\n",
        "    xm.save(\n",
        "        model.state_dict(), f'model_5e_{datetime.now().strftime(\"%Y%m%d-%H%M\")}.pth'\n",
        "    )\n"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Tbu-8P_e8Pzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b76cc1-8738-4aee-da27-d12920c205fd"
      },
      "cell_type": "code",
      "source": [
        "# Training 시작\n",
        "def _mp_fn(rank, flags):\n",
        "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
        "    a = _run()\n",
        "\n",
        "\n",
        "# Run\n",
        "FLAGS = {}\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method=\"fork\")"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1838953060.py:62: DeprecationWarning: Use torch_xla.device instead\n",
            "  device = xm.xla_device()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INITIALIZING TRAINING ON 1 TPU CORES\n",
            "Start Time: 2025-11-06 05:05:08.597332\n",
            "==================================================\n",
            "EPOCH 1 - TRAINING...\n",
            "\tBATCH 1/340 - LOSS: 2.015625\n",
            "\tBATCH 21/340 - LOSS: 1.8515625\n",
            "\tBATCH 41/340 - LOSS: 1.5546875\n",
            "\tBATCH 61/340 - LOSS: 1.2734375\n",
            "\tBATCH 81/340 - LOSS: 1.3828125\n",
            "\tBATCH 101/340 - LOSS: 0.99609375\n",
            "\tBATCH 121/340 - LOSS: 1.15625\n",
            "\tBATCH 141/340 - LOSS: 1.2578125\n",
            "\tBATCH 161/340 - LOSS: 0.94921875\n",
            "\tBATCH 181/340 - LOSS: 1.171875\n",
            "\tBATCH 201/340 - LOSS: 1.0078125\n",
            "\tBATCH 221/340 - LOSS: 0.734375\n",
            "\tBATCH 241/340 - LOSS: 1.2265625\n",
            "\tBATCH 261/340 - LOSS: 1.015625\n",
            "\tBATCH 281/340 - LOSS: 0.98046875\n",
            "\tBATCH 301/340 - LOSS: 0.93359375\n",
            "\tBATCH 321/340 - LOSS: 1.1171875\n",
            "\n",
            "\t[TRAIN] EPOCH 1 - LOSS: 1.1328125, ACCURACY: 0.61328125\n",
            "\n",
            "EPOCH 1 - VALIDATING...\n",
            "\t[VALID] LOSS: 0.9921875, ACCURACY: 0.67578125\n",
            "\n",
            "Execution time: 0:01:18.276088\n",
            "Saving Model\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V6E1"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}