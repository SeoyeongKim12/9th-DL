{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec**"
      ],
      "metadata": {
        "id": "xcLelalG0Exp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Skip-gram"
      ],
      "metadata": {
        "id": "a4H1Wn638w5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "embedding = torch.nn.Embedding(\n",
        "    num_embeddings,\n",
        "    embedding_dim,\n",
        "    padding_idx=None,\n",
        "    max_norm=None,\n",
        "    norm_type=2.0\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LpXN4vQm9iO_",
        "outputId": "b85efbc0-5e55-49cc-aae8-7890c21bb78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nembedding = torch.nn.Embedding(\\n    num_embeddings,\\n    embedding_dim,\\n    padding_idx=None,\\n    max_norm=None,\\n    norm_type=2.0\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#기본 Skip-gram 클래스\n",
        "from torch import nn\n",
        "\n",
        "class VanillaSkipgram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding=nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim\n",
        "        )\n",
        "        self.linear=nn.Linear(\n",
        "            in_features=embedding_dim,\n",
        "            out_features=vocab_size\n",
        "        )\n",
        "    def forward(self, input_ids):\n",
        "        embeddings=self.embedding(input_ids)\n",
        "        output=self.linear(embeddings)\n",
        "        return output"
      ],
      "metadata": {
        "id": "4CHNMK0t2drD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Korpora konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43APYYcv38n1",
        "outputId": "5f02e2e4-c939-4f8f-ad25-dc92dceb19b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Korpora in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.11/dist-packages (from Korpora) (0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.0.1)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (1.5.2)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#영화 리뷰 데이터세트 전처리\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "corpus=Korpora.load(\"nsmc\")\n",
        "corpus=pd.DataFrame(corpus.test)\n",
        "\n",
        "tokenizer=Okt()\n",
        "tokens=[tokenizer.morphs(review) for review in corpus.text]\n",
        "print(tokens[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gLc3kcZ3evZ",
        "outputId": "31d5f936-cbcb-4d96-ad92-40ae9183a4e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n",
            "[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 사전 구축\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter=Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab=special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "vocab=build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])\n",
        "token_to_id={token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token={idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-anLaFc4QrM",
        "outputId": "5c570580-eb3e-4ec2-f0d9-49faff465928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '.', '이', '영화', '의', '..', '가', '에', '...', '을']\n",
            "5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Skip-gram의 단어 쌍 추출\n",
        "def get_word_pairs(tokens, window_size):\n",
        "    pairs=[]\n",
        "    for sentence in tokens:\n",
        "        sentence_length=len(sentence)\n",
        "        for idx, center_word in enumerate(sentence):\n",
        "            window_start=max(0, idx - window_size)\n",
        "            window_end=min(sentence_length, idx + window_size + 1)\n",
        "            center_word=sentence[idx]\n",
        "            context_words=sentence[window_start:idx] + sentence[idx+1:window_end]\n",
        "        for context_word in context_words:\n",
        "            pairs.append([center_word, context_word])\n",
        "    return pairs\n",
        "\n",
        "word_pairs=get_word_pairs(tokens, window_size=2)\n",
        "print(word_pairs[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-zgQn6m5TvY",
        "outputId": "69e40e6a-0cd3-4f7c-883b-dc286f1af1be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['ㅋ', '굳'], ['아니잖아', '더'], ['아니잖아', '더욱'], ['....', '보기'], ['....', '에는']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#인덱스 쌍 변환\n",
        "def get_index_pairs(word_pairs, token_to_id):\n",
        "    pairs=[]\n",
        "    unk_index=token_to_id[\"<unk>\"]\n",
        "    for word_pair in word_pairs:\n",
        "        centor_word, context_word=word_pair\n",
        "        centor_index=token_to_id.get(centor_word, unk_index)\n",
        "        context_index=token_to_id.get(context_word, unk_index)\n",
        "        pairs.append([centor_index, context_index])\n",
        "    return pairs\n",
        "\n",
        "index_pairs=get_index_pairs(word_pairs, token_to_id)\n",
        "print(index_pairs[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUMACzpw50LR",
        "outputId": "32624254-c757-40e4-e3bb-3a494ad219cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[100, 595], [2596, 57], [2596, 903], [48, 160], [48, 246]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터로더 적용\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "index_pairs=torch.tensor(index_pairs)\n",
        "center_indexs=index_pairs[:, 0]\n",
        "context_indexs=index_pairs[:, 1]\n",
        "\n",
        "dataset=TensorDataset(center_indexs, context_indexs)\n",
        "dataloader=DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "S688VTXv6Zoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Skip-gram 모델 준비 작업\n",
        "from torch import optim\n",
        "\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "word2vec=VanillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)\n",
        "criterion=nn.CrossEntropyLoss().to(device)\n",
        "optimizer=optim.SGD(word2vec.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "fYyNSaD56pJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습\n",
        "for epoch in range(10):\n",
        "    cost=0.0\n",
        "    for input_ids, target_ids in dataloader:\n",
        "        input_ids=input_ids.to(device)\n",
        "        target_ids=target_ids.to(device)\n",
        "\n",
        "        logits=word2vec(input_ids)\n",
        "        loss=criterion(logits, target_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cost+=loss\n",
        "\n",
        "    cost=cost / len(dataloader)\n",
        "    print(f\"Epoch: {epoch+1:4d}, Cost: {cost:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIICN1Ql7OII",
        "outputId": "befa5e26-414d-43bc-91c1-20d9a048db34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:    1, Cost: 6.673\n",
            "Epoch:    2, Cost: 6.069\n",
            "Epoch:    3, Cost: 5.851\n",
            "Epoch:    4, Cost: 5.709\n",
            "Epoch:    5, Cost: 5.606\n",
            "Epoch:    6, Cost: 5.523\n",
            "Epoch:    7, Cost: 5.455\n",
            "Epoch:    8, Cost: 5.397\n",
            "Epoch:    9, Cost: 5.348\n",
            "Epoch:   10, Cost: 5.303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩 값 추출\n",
        "token_to_embedding=dict()\n",
        "embedding_matrix=word2vec.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "for word, embedding in zip(vocab, embedding_matrix):\n",
        "    token_to_embedding[word]=embedding\n",
        "\n",
        "index=30\n",
        "token=vocab[30]\n",
        "token_embedding=token_to_embedding[token]\n",
        "print(token)\n",
        "print(token_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72DtW7Y67i6l",
        "outputId": "6944baf0-ff2f-4149-ef57-c20cc67cbe1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "연기\n",
            "[ 0.5066367  -1.7869946   1.6448294   0.89922553  0.20326823 -0.2628825\n",
            " -2.2654488   0.9203757   1.2920295   0.5848853   1.574148   -0.8532711\n",
            " -0.00398266  0.12257556  1.3608975   0.4857879   0.18131097 -0.45535228\n",
            "  0.29566503  0.2342968  -0.25485453 -0.6742077  -0.9360183  -1.2431601\n",
            "  0.85525835  0.48338258 -0.44903103  1.1429794   1.7266386   1.6315706\n",
            " -0.07791245  0.53372175 -0.74694866  1.5385375  -0.66876096 -0.5279956\n",
            "  0.41458336 -0.31082094 -0.46849412 -0.35467234 -0.45798895 -0.01768018\n",
            "  0.08889236 -0.6118245  -1.0128369  -0.10165706  0.07623158  1.1023722\n",
            "  2.1637952   1.537462   -0.5787339  -0.22282283  0.52543867  1.4715426\n",
            " -0.4562049   0.2290246  -0.5590634  -1.545982    0.1516511  -0.3736755\n",
            "  0.1028283   0.8478788   0.70460135 -1.9054956  -0.6678803   0.9613172\n",
            "  1.2960109   0.50879854 -1.0169264  -0.18116437  0.54497206  1.3762122\n",
            " -0.61338836  0.25766402 -0.14560688  0.77785456  1.2299052  -1.3991659\n",
            " -1.448754    1.6916965   1.5390346   0.5739683   0.1962339  -0.97485226\n",
            " -0.37552905 -1.1112008   2.0988483   0.6002333   0.14910635  1.0739013\n",
            "  0.8391308   1.1273448   0.37204847 -0.26410782 -0.55097955  0.9716755\n",
            " -0.26326713 -0.8724446  -0.7016738  -0.6451709   0.73893374  1.754995\n",
            " -1.4780123   0.8665135   0.64953667 -0.8599826  -0.08895208  0.03252333\n",
            "  0.44758725 -0.31592456  0.05555493  2.0129223  -1.495324   -1.5964768\n",
            "  0.51396036 -0.7916325  -1.7786462  -0.00377856 -1.4226458   0.81966454\n",
            " -1.5831589  -1.608457    1.3054955  -0.6247471  -0.39077473 -0.7495814\n",
            " -0.7388409   0.6876851 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 임베딩 유사도 계산\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    cosine=np.dot(b, a) / (norm(b, axis=1)*norm(a))\n",
        "    return cosine\n",
        "\n",
        "def top_n_index(cosine_matrix, n):\n",
        "  closest_indexes=cosine_matrix.argsort()[::-1]\n",
        "  top_n=closest_indexes[1:n+1]\n",
        "  return top_n\n",
        "\n",
        "cosine_matrix=cosine_similarity(token_embedding, embedding_matrix)\n",
        "top_n=top_n_index(cosine_matrix, n=5)\n",
        "\n",
        "print(f\"{token}와 가장 유사한 5개 단어\")\n",
        "for index in top_n:\n",
        "    print(f\"{id_to_token[index]} - 유사도: {cosine_matrix[index]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMq1vAJH8Jta",
        "outputId": "46460034-99a8-4a7c-833b-a70d8a0f10bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "연기와 가장 유사한 5개 단어\n",
            "솔직한 - 유사도: 0.3102\n",
            "담담하게 - 유사도: 0.2988\n",
            "피디 - 유사도: 0.2815\n",
            "주는데 - 유사도: 0.2758\n",
            "에게도 - 유사도: 0.2723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Gensim"
      ],
      "metadata": {
        "id": "LvQh7e3z8yyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDAtM83w80B4",
        "outputId": "f520622e-71e4-4b5c-ff9b-a3bd6389ec60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "word2vec=gensim.models.Word2Vec(\n",
        "    sentences=None,\n",
        "    corpus_file=None,\n",
        "    vector_size=100,\n",
        "    alpha=0.025,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    workers=3,\n",
        "    sg=0,\n",
        "    hs=0,\n",
        "    cbow_mean=1,\n",
        "    negative=5,\n",
        "    ns_exponent=0.75,\n",
        "    max_final_vocab=None,\n",
        "    epochs=5,\n",
        "    batch_words=10000\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "KRb6nmVb-H_E",
        "outputId": "954b9a2f-9ccc-4467-80c6-55cb5c6f5c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nword2vec=gensim.models.Word2Vec(\\n    sentences=None,\\n    corpus_file=None,\\n    vector_size=100,\\n    alpha=0.025,\\n    window=5,\\n    min_count=5,\\n    workers=3,\\n    sg=0,\\n    hs=0,\\n    cbow_mean=1,\\n    negative=5,\\n    ns_exponent=0.75,\\n    max_final_vocab=None,\\n    epochs=5,\\n    batch_words=10000\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy gensim\n",
        "!pip install numpy gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOOGIJy9_9Vo",
        "outputId": "478e38b7-66dd-4809-d92b-f033f6a6ca7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: gensim 4.3.3\n",
            "Uninstalling gensim-4.3.3:\n",
            "  Successfully uninstalled gensim-4.3.3\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy, gensim\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word2Vec 모델 학습\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "word2vec=Word2Vec(\n",
        "    sentences=tokens,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    sg=1,\n",
        "    epochs=3,\n",
        "    max_final_vocab=10000\n",
        ")\n",
        "\n",
        "#word2vec.save(\"../models/word2vec.model\")\n",
        "#word2vec=Word2Vec.load(\"../models/word2vec.model\")"
      ],
      "metadata": {
        "id": "czS31sOo-KcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩 추출 및 유사도 계산\n",
        "word=\"연기\"\n",
        "print(word2vec.wv[word])\n",
        "print(word2vec.wv.most_similar(word, topn=5))\n",
        "print(word2vec.wv.similarity(w1=word, w2=\"연기력\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arsGrrrPJUaY",
        "outputId": "e61da691-4295-48a1-f5d3-5d6688db2368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.35138252 -0.09575734  0.01078036  0.3296205  -0.06404735 -0.04847586\n",
            " -0.02165333 -0.17068434 -0.49626878  0.46267617  0.02163552 -0.27002376\n",
            " -0.32610792  0.03496901  0.03636095 -0.0592006  -0.21744326  0.07258674\n",
            " -0.21599753  0.2781314   0.6629464   0.16208528 -0.15428735 -0.18397978\n",
            " -0.23799847 -0.18186544 -0.265073   -0.19382411  0.06363969 -0.18098818\n",
            " -0.32142508  0.33520392  0.30456883 -0.14357653  0.12910873 -0.28297436\n",
            "  0.1370951  -0.17673874 -0.07532267 -0.49993727 -0.06604859  0.11669078\n",
            " -0.07926828 -0.4765588  -0.31296754  0.25067112 -0.25892273 -0.27944908\n",
            "  0.24863765  0.00984355  0.7199508   0.31880382  0.08363467  0.21541536\n",
            " -0.3441615   0.17021452  0.29313296  0.36821678 -0.00770938  0.3297394\n",
            "  0.19861107 -0.35226375  0.22778349 -0.10607314 -0.33860916  0.52227736\n",
            " -0.04290876  0.11622152  0.4018581  -0.31056097 -0.38013083 -0.16095804\n",
            " -0.3973865   0.09471136 -0.03457201 -0.13507968 -0.27435318 -0.28389287\n",
            " -0.15536116  0.25298804  0.15752414  0.17916612  0.41358763  0.6221476\n",
            "  0.18550067  0.33811292  0.2184238  -0.432586    0.00089814 -0.2188944\n",
            " -0.31943208 -0.21105033  0.37351224  0.16313802  0.05644973 -0.0447598\n",
            " -0.02202114 -0.2823111  -0.2226388  -0.32730502 -0.47908172 -0.02003665\n",
            "  0.2793615  -0.01589685 -0.19266582  0.20708407  0.37510195 -0.01920766\n",
            "  0.11247467 -0.6052304   0.23943883 -0.08222314 -0.34846064  0.17854576\n",
            "  0.31252727 -0.1266669   0.35874397  0.5094998  -0.10555429  0.19748501\n",
            " -0.09975012 -0.28610674  0.00572789 -0.316865   -0.08828755  0.10650007\n",
            " -0.4313594  -0.53897107]\n",
            "[('연기력', 0.7749751210212708), ('캐스팅', 0.7468073964118958), ('연기자', 0.7162203788757324), ('몸매', 0.7069709300994873), ('조연', 0.70069420337677)]\n",
            "0.77497524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**fastText**"
      ],
      "metadata": {
        "id": "VW_FjuVvJjNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "fasttext=gensim.models.FastText(\n",
        "    sentences=None,\n",
        "    corpus_file=None,\n",
        "    vector_size=100,\n",
        "    alpha=0.025,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    workers=3,\n",
        "    sg=0,\n",
        "    hs=0,\n",
        "    cbow_mean=1,\n",
        "    negative=5,\n",
        "    ns_exponent=0.75,\n",
        "    max_final_vocab=None,\n",
        "    epochs=5,\n",
        "    batch_words=10000,\n",
        "    min_n=3,\n",
        "    max_n=6\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "yqGfC9wcJlen",
        "outputId": "29bc4ec3-f19c-41c6-982d-c28efe68ae5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfasttext=gensim.models.FastText(\\n    sentences=None,\\n    corpus_file=None,\\n    vector_size=100,\\n    alpha=0.025,\\n    window=5,\\n    min_count=5,\\n    workers=3,\\n    sg=0,\\n    hs=0,\\n    cbow_mean=1,\\n    negative=5,\\n    ns_exponent=0.75,\\n    max_final_vocab=None,\\n    epochs=5,\\n    batch_words=10000,\\n    min_n=3,\\n    max_n=6\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KorNLI 데이터세트 전처리\n",
        "from Korpora import Korpora\n",
        "\n",
        "corpus=Korpora.load(\"kornli\")\n",
        "corpus_texts=corpus.get_all_texts() + corpus.get_all_pairs()\n",
        "tokens=[sentence.split() for sentence in corpus_texts]\n",
        "\n",
        "print(tokens[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru9okvk6LLCP",
        "outputId": "3ba93525-c64b-4b64-ba2e-e13e7f6df392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : KakaoBrain\n",
            "    Repository : https://github.com/kakaobrain/KorNLUDatasets\n",
            "    References :\n",
            "        - Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). KorNLI and KorSTS: New Benchmark\n",
            "           Datasets for Korean Natural Language Understanding. arXiv preprint arXiv:2004.03289.\n",
            "           (https://arxiv.org/abs/2004.03289)\n",
            "\n",
            "    This is the dataset repository for our paper\n",
            "    \"KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding.\"\n",
            "    (https://arxiv.org/abs/2004.03289)\n",
            "    We introduce KorNLI and KorSTS, which are NLI and STS datasets in Korean.\n",
            "\n",
            "    # License\n",
            "    Creative Commons Attribution-ShareAlike license (CC BY-SA 4.0)\n",
            "    Details in https://creativecommons.org/licenses/by-sa/4.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[kornli] download multinli.train.ko.tsv: 83.6MB [00:02, 28.7MB/s]                            \n",
            "[kornli] download snli_1.0_train.ko.tsv: 78.5MB [00:00, 146MB/s]                            \n",
            "[kornli] download xnli.dev.ko.tsv: 516kB [00:00, 1.89MB/s]                            \n",
            "[kornli] download xnli.test.ko.tsv: 1.04MB [00:00, 3.59MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['개념적으로', '크림', '스키밍은', '제품과', '지리라는', '두', '가지', '기본', '차원을', '가지고', '있다.'], ['시즌', '중에', '알고', '있는', '거', '알아?', '네', '레벨에서', '다음', '레벨로', '잃어버리는', '거야', '브레이브스가', '모팀을', '떠올리기로', '결정하면', '브레이브스가', '트리플', 'A에서', '한', '남자를', '떠올리기로', '결정하면', '더블', 'A가', '그를', '대신하러', '올라가고', 'A', '한', '명이', '그를', '대신하러', '올라간다.'], ['우리', '번호', '중', '하나가', '당신의', '지시를', '세밀하게', '수행할', '것이다.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fastText 모델 실습\n",
        "from gensim.models import FastText\n",
        "\n",
        "fastText=FastText(\n",
        "    sentences=tokens,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    sg=1,\n",
        "    epochs=3,\n",
        "    min_n=2,\n",
        "    max_n=6\n",
        ")\n",
        "\n",
        "#fastText.save(\"../models/fastText.model\")\n",
        "#fastText=FastText.load(\"../models/fastText.model\")"
      ],
      "metadata": {
        "id": "WcBxJ2YRL5Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fastText OOV 처리\n",
        "oov_token=\"사랑해요\"\n",
        "oov_vector=fastText.wv[oov_token]\n",
        "\n",
        "print(oov_token in fastText.wv.index_to_key)\n",
        "print(fastText.wv.most_similar(oov_vector, topn=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRM9GiobMIRR",
        "outputId": "fe91150e-807f-44f1-c268-3227416f19a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "[('사랑해', 0.9067966938018799), ('사랑', 0.8586940169334412), ('사랑한', 0.8522242903709412), ('사랑해서', 0.843116819858551), ('사랑해.', 0.8415320515632629)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**순환신경망**"
      ],
      "metadata": {
        "id": "h8N1TZe6MVYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "rnn=torch.nn.RNN(\n",
        "    input_size,\n",
        "    hidden_size,\n",
        "    num_layers=1,\n",
        "    nonlinearity=\"tanh\",\n",
        "    bias=False,\n",
        "    batch_first=True,\n",
        "    dropout=0,\n",
        "    bidirectional=False\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SVu4kR9KRllb",
        "outputId": "906b82f9-2a38-4403-d706-abefc39c74c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nrnn=torch.nn.RNN(\\n    input_size,\\n    hidden_size,\\n    num_layers=1,\\n    nonlinearity=\"tanh\",\\n    bias=False,\\n    batch_first=True,\\n    dropout=0,\\n    bidirectional=False\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#양방향 다층 신경망\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "input_size=128\n",
        "output_size=256\n",
        "num_layers=3\n",
        "bidirectional=True\n",
        "\n",
        "model=nn.RNN(\n",
        "    input_size=input_size,\n",
        "    hidden_size=output_size,\n",
        "    num_layers=num_layers,\n",
        "    nonlinearity='tanh',\n",
        "    batch_first=True,\n",
        "    bidirectional=bidirectional,\n",
        ")\n",
        "\n",
        "batch_size=4\n",
        "sequence_len=6\n",
        "\n",
        "inputs=torch.randn(batch_size, sequence_len, input_size)\n",
        "h_0=torch.rand(num_layers*(int(bidirectional) + 1), batch_size, output_size)\n",
        "\n",
        "outputs, hidden=model(inputs, h_0)\n",
        "print(outputs.shape)\n",
        "print(hidden.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaQhlxPkSVUS",
        "outputId": "3bfeb5c3-1d62-4b11-ea6c-db2cc3ad107a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 6, 512])\n",
            "torch.Size([6, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "lstm=torch.nn.LSTM(\n",
        "    input_size,\n",
        "    hidden_size,\n",
        "    num_layers=1,\n",
        "    bias=False,\n",
        "    batch_first=True,\n",
        "    dropout=0,\n",
        "    bidirectional=False,\n",
        "    proj_size=0\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wB-wTgzbVO5D",
        "outputId": "c7642e44-24e6-405a-a989-ea1b461120af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nlstm=torch.nn.LSTM(\\n    input_size,\\n    hidden_size,\\n    num_layers=1,\\n    bias=False,\\n    batch_first=True,\\n    dropout=0,\\n    bidirectional=False,\\n    proj_size=0\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#양방향 다층 장단기 메모리\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "input_size=128\n",
        "ouput_size=256\n",
        "num_layers=3\n",
        "bidirectional=True\n",
        "proj_size=64\n",
        "\n",
        "model=nn.LSTM(\n",
        "    input_size=input_size,\n",
        "    hidden_size=ouput_size,\n",
        "    num_layers=num_layers,\n",
        "    batch_first=True,\n",
        "    bidirectional=bidirectional,\n",
        "    proj_size=proj_size,\n",
        ")\n",
        "\n",
        "batch_size=4\n",
        "sequence_len=6\n",
        "\n",
        "inputs=torch.randn(batch_size, sequence_len, input_size)\n",
        "h_0=torch.rand(\n",
        "    num_layers*(int(bidirectional) + 1),\n",
        "    batch_size,\n",
        "    proj_size if proj_size > 0 else ouput_size,\n",
        ")\n",
        "c_0=torch.rand(num_layers*(int(bidirectional) + 1), batch_size, ouput_size)\n",
        "\n",
        "outputs, (h_n, c_n)=model(inputs, (h_0, c_0))\n",
        "\n",
        "print(outputs.shape)\n",
        "print(h_n.shape)\n",
        "print(c_n.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q88R1cUiVoub",
        "outputId": "34e27663-2cf4-49a9-9e52-2f98400e5ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 6, 128])\n",
            "torch.Size([6, 4, 64])\n",
            "torch.Size([6, 4, 256])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: LSTM with projections is not supported with oneDNN. Using default implementation. (Triggered internally at /pytorch/aten/src/ATen/native/RNN.cpp:1474.)\n",
            "  result = _VF.lstm(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#문장 분류 모델\n",
        "from torch import nn\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "  def __init__(\n",
        "          self,\n",
        "          n_vocab,\n",
        "          hidden_dim,\n",
        "          embedding_dim,\n",
        "          n_layers,\n",
        "          dropout=0.5,\n",
        "          bidirectional=True,\n",
        "          model_type='lstm'\n",
        "    ):\n",
        "          super().__init__()\n",
        "\n",
        "          self.embedding=nn.Embedding(\n",
        "              num_embeddings=n_vocab,\n",
        "              embedding_dim=embedding_dim,\n",
        "              padding_idx=0\n",
        "          )\n",
        "          if model_type == \"rnn\":\n",
        "            self.model = nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "          elif model_type == \"lstm\":\n",
        "            self.model = nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "          if bidirectional:\n",
        "            self.classifier=nn.Linear(hidden_dim*2, 1)\n",
        "          else:\n",
        "            self.classifier=nn.Linear(hidden_dim, 1)\n",
        "          self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    embeddings=self.embedding(inputs)\n",
        "    output, _=self.model(embeddings)\n",
        "    last_output=output[:, -1, :]\n",
        "    last_output=self.dropout(last_output)\n",
        "    logits=self.classifier(last_output)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "GYTIAHNdZA59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터세트 불러오기\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "corpus=Korpora.load(\"nsmc\")\n",
        "corpus_df=pd.DataFrame(corpus.test)\n",
        "\n",
        "train=corpus_df.sample(frac=0.9, random_state=42)\n",
        "test=corpus_df.drop(train.index)\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(\"Training Data Size:\", len(train))\n",
        "print(\"Testing Data Size:\", len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O88dnZViamh-",
        "outputId": "f958a061-322b-4350-873a-9710a5640a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n",
            "|       | text                                                                                     |   label |\n",
            "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
            "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
            "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
            "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
            "| 12447 | 잔잔 격동                                                                                |       1 |\n",
            "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
            "Training Data Size: 45000\n",
            "Testing Data Size: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 토큰화 및 단어 사전 구축\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter=Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab=special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "tokenizer=Okt()\n",
        "train_tokens=[tokenizer.morphs(review) for review in train.text]\n",
        "test_tokens=[tokenizer.morphs(review) for review in test.text]\n",
        "\n",
        "vocab=build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "token_to_id={token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token={idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFYXO2oFa2bK",
        "outputId": "b8c2519d-38a7-42e8-af7e-c79075c968eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
            "5002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#정수 인코딩 및 패딩\n",
        "import numpy as np\n",
        "\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = list()\n",
        "    for sequence in sequences:\n",
        "        sequence=sequence[:max_length]\n",
        "        pad_length=max_length - len(sequence)\n",
        "        padded_sequence=sequence + [pad_value] * pad_length\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n",
        "\n",
        "unk_id=token_to_id[\"<unk>\"]\n",
        "train_ids=[\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
        "]\n",
        "test_ids=[\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
        "]\n",
        "\n",
        "max_length=32\n",
        "pad_id=token_to_id[\"<pad>\"]\n",
        "train_ids=pad_sequences(train_ids, max_length, pad_id)\n",
        "test_ids=pad_sequences(test_ids, max_length, pad_id)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(test_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgoF0hw3bzgQ",
        "outputId": "8bb737f2-9561-4399-a019-7a4820fdbb45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
            " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
            "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터로더 적용\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "train_ids=torch.tensor(train_ids)\n",
        "test_ids=torch.tensor(test_ids)\n",
        "\n",
        "train_labels=torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels=torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset=TensorDataset(train_ids, train_labels)\n",
        "test_dataset=TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader=DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader=DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "cEKWGvI5cXoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#손실 함수와 최적화 함수 정의\n",
        "from torch import optim\n",
        "\n",
        "n_vocab=len(token_to_id)\n",
        "hidden_dim=64\n",
        "embedding_dim=128\n",
        "n_layers=2\n",
        "\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "classifier=SentenceClassifier(\n",
        "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
        ").to(device)\n",
        "criterion=nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer=optim.RMSprop(classifier.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "fbDWNeggcvcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습 및 테스트\n",
        "def train(model, datasets, criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses=list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids=input_ids.to(device)\n",
        "        labels=labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits=model(input_ids)\n",
        "        loss=criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f\"Train Loss {step}: {np.mean(losses)}\")\n",
        "\n",
        "\n",
        "def test(model, datasets, criterion, device):\n",
        "    model.eval()\n",
        "    losses=list()\n",
        "    corrects=list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids=input_ids.to(device)\n",
        "        labels=labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits=model(input_ids)\n",
        "        loss=criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "        yhat=torch.sigmoid(logits)>.5\n",
        "        corrects.extend(\n",
        "            torch.eq(yhat, labels).cpu().tolist()\n",
        "        )\n",
        "\n",
        "    print(f\"Val Loss: {np.mean(losses)}, Val Accuracy: {np.mean(corrects)}\")\n",
        "\n",
        "\n",
        "epochs=5\n",
        "interval=500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VofIPMlfdtMt",
        "outputId": "26ffe5f8-37cb-40d8-c6c1-e4b9b8a9e0f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss 0: 0.6964503526687622\n",
            "Train Loss 500: 0.6935324485668403\n",
            "Train Loss 1000: 0.6929982728534169\n",
            "Train Loss 1500: 0.6757427425800682\n",
            "Train Loss 2000: 0.6529014282289712\n",
            "Train Loss 2500: 0.6295936134446863\n",
            "Val Loss: 0.4977589296266294, Val Accuracy: 0.7594\n",
            "Train Loss 0: 0.25559377670288086\n",
            "Train Loss 500: 0.4828844573088511\n",
            "Train Loss 1000: 0.47568262628206126\n",
            "Train Loss 1500: 0.46985076532651393\n",
            "Train Loss 2000: 0.4642885667660545\n",
            "Train Loss 2500: 0.4590223305752543\n",
            "Val Loss: 0.4423000474516957, Val Accuracy: 0.7796\n",
            "Train Loss 0: 0.5183154344558716\n",
            "Train Loss 500: 0.39548741858043596\n",
            "Train Loss 1000: 0.40511455912362565\n",
            "Train Loss 1500: 0.4021449322445563\n",
            "Train Loss 2000: 0.3973186500575291\n",
            "Train Loss 2500: 0.3979281688161918\n",
            "Val Loss: 0.4268634514972425, Val Accuracy: 0.8042\n",
            "Train Loss 0: 0.28801777958869934\n",
            "Train Loss 500: 0.34975270482296955\n",
            "Train Loss 1000: 0.3506694947342415\n",
            "Train Loss 1500: 0.35307166569039156\n",
            "Train Loss 2000: 0.3553510350563835\n",
            "Train Loss 2500: 0.3552669104678733\n",
            "Val Loss: 0.39583747074626885, Val Accuracy: 0.816\n",
            "Train Loss 0: 0.24581637978553772\n",
            "Train Loss 500: 0.31381901082937824\n",
            "Train Loss 1000: 0.31179192217139456\n",
            "Train Loss 1500: 0.31286663837050854\n",
            "Train Loss 2000: 0.31683317720257065\n",
            "Train Loss 2500: 0.31865561872822007\n",
            "Val Loss: 0.39981595174477885, Val Accuracy: 0.8194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#학습된 모델로부터 임베딩 추출\n",
        "token_to_embedding=dict()\n",
        "embedding_matrix=classifier.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "for word, emb in zip(vocab, embedding_matrix):\n",
        "  token_to_embedding[word]=emb\n",
        "\n",
        "token=vocab[1000]\n",
        "print(token, token_to_embedding[token])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPVLcF54eQq5",
        "outputId": "6a66d52e-1c4e-43e7-b71c-5a95bd4a25ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "보고싶다 [-6.5684944e-02  9.8038960e-01  6.7936951e-01 -7.2716779e-01\n",
            " -5.3028971e-01 -9.6538115e-01 -1.7423812e-01 -3.5035372e-01\n",
            "  1.3865409e+00  2.3131403e-01 -4.4144753e-01 -1.2203298e+00\n",
            "  2.0606885e+00 -2.7916052e+00  1.1546307e+00 -1.7085491e+00\n",
            "  1.1573409e+00 -4.6776158e-01 -2.0752938e+00  6.1785638e-01\n",
            " -7.0677716e-01  5.2808112e-01  6.3309377e-01 -2.3409292e-01\n",
            " -1.8632468e+00  4.0347210e-01 -5.7640767e-01  7.4862212e-01\n",
            "  1.2225380e+00  2.2368523e-01  1.1497176e+00  1.8548336e+00\n",
            "  8.0419749e-01 -8.8360643e-01  6.9644058e-01 -1.1349346e-01\n",
            "  1.1079184e+00 -6.8089902e-01 -1.0229847e+00  1.0506445e+00\n",
            " -1.2056209e+00 -9.9437767e-01  9.1950691e-01 -6.9805789e-01\n",
            "  1.1958611e+00  2.7181113e-01 -3.1870151e-01 -3.0849561e-01\n",
            " -2.5942824e+00  1.0716674e+00 -9.4971126e-01 -8.3079308e-01\n",
            "  7.6429561e-02 -2.4256243e-01 -1.0329024e+00  1.1752014e+00\n",
            "  1.9150287e+00 -1.4501592e+00 -1.5901750e+00  1.1315821e+00\n",
            " -1.6957401e+00 -1.9149452e+00 -5.6553084e-01  5.6782079e-01\n",
            " -4.4920194e-04  1.4149003e+00  8.7266505e-01  2.0772412e+00\n",
            "  1.2786598e+00  7.6587826e-01 -8.5192434e-02  1.8521341e+00\n",
            " -1.4749981e+00 -1.2489078e+00 -4.5575628e-01  1.2899354e+00\n",
            "  3.1348032e-01 -1.9515306e-01  3.8968956e-01  1.3844488e+00\n",
            " -7.9368263e-01 -8.0734370e-03  2.2521465e-01  1.0968006e-01\n",
            " -1.5156018e+00 -8.0986506e-01 -8.3361453e-01  1.1193068e+00\n",
            " -4.3793705e-01  3.9498204e-01  2.3841476e+00 -1.5487059e+00\n",
            "  4.9523312e-01  8.0209225e-01  1.5878787e+00 -7.4467915e-01\n",
            "  9.4528839e-02  4.3920779e-01 -1.5200907e+00  3.2347551e-01\n",
            " -1.9086917e-01 -6.1095154e-01 -3.2397184e-01 -9.2138052e-01\n",
            "  8.8978016e-01 -5.0819772e-01 -8.6110896e-01 -1.5372097e+00\n",
            "  1.2831959e+00 -2.6665300e-01 -7.7396512e-01 -9.6083397e-01\n",
            " -1.1722497e+00 -1.3830720e+00  2.2393115e-01 -6.7019993e-01\n",
            " -2.9492071e-02  5.2143730e-02  1.0197195e+00 -5.4526228e-01\n",
            "  1.2313380e+00  1.4660321e+00  1.4572896e+00 -1.0070988e-01\n",
            "  3.5677081e-01  2.2824255e-01  3.7663844e-01 -1.5593612e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#사전 학습된 모델로 임베딩 계층 초기화\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "word2vec=Word2Vec.load(\"../models/word2vec.model\")\n",
        "init_embeddings=np.zeros((n_vocab, embedding_dim))\n",
        "\n",
        "for index, token in id_to_token.items():\n",
        "    if token not in [\"<pad>\", \"<unk>\"]:\n",
        "        init_embeddings[index]=word2vec.wv[token]\n",
        "\n",
        "embedding_layer=nn.Embedding.from_pretrained(\n",
        "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "aQbLoHlLelNv",
        "outputId": "4c7ebd1d-850a-4111-a259-c28377dee937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#사전 학습된 모델로 임베딩 계층 초기화\\nfrom gensim.models import Word2Vec\\n\\nword2vec=Word2Vec.load(\"../models/word2vec.model\")\\ninit_embeddings=np.zeros((n_vocab, embedding_dim))\\n\\nfor index, token in id_to_token.items():\\n    if token not in [\"<pad>\", \"<unk>\"]:\\n        init_embeddings[index]=word2vec.wv[token]\\n\\nembedding_layer=nn.Embedding.from_pretrained(\\n    torch.tensor(init_embeddings, dtype=torch.float32)\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#사전 학습된 임베딩 계층 적용\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab,\n",
        "        hidden_dim,\n",
        "        embedding_dim,\n",
        "        n_layers,\n",
        "        dropout=0.5,\n",
        "        bidirectional=True,\n",
        "        model_type=\"lstm\",\n",
        "        pretrained_embedding=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if pretrained_embedding is not None:\n",
        "            self.embedding=nn.Embedding.from_pretrained(\n",
        "                torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
        "            )\n",
        "        else:\n",
        "            self.embedding=nn.Embedding(\n",
        "                num_embeddings=n_vocab,\n",
        "                embedding_dim=embedding_dim,\n",
        "                padding_idx=0\n",
        "            )\n",
        "\n",
        "        if model_type == \"rnn\":\n",
        "            self.model=nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        elif model_type == \"lstm\":\n",
        "            self.model=nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "        if bidirectional:\n",
        "            self.classifier=nn.Linear(hidden_dim * 2, 1)\n",
        "        else:\n",
        "            self.classifier=nn.Linear(hidden_dim, 1)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings=self.embedding(inputs)\n",
        "        output, _=self.model(embeddings)\n",
        "        last_output=output[:, -1, :]\n",
        "        last_output=self.dropout(last_output)\n",
        "        logits=self.classifier(last_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "AGdiNPHef1GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#사전 학습된 임베딩을 사용한 모델 학습\n",
        "classifier=SentenceClassifier(\n",
        "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim,\n",
        "    n_layers=n_layers, pretrained_embedding=init_embeddings\n",
        ").to(device)\n",
        "criterion=nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer=optim.RMSprop(classifier.parameters(), lr=0.001)\n",
        "\n",
        "epochs=5\n",
        "interval=500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "nDQQLndvgOdx",
        "outputId": "3770d6e9-4ad6-4e31-dda1-a3d36999f8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#사전 학습된 임베딩을 사용한 모델 학습\\nclassifier=SentenceClassifier(\\n    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim,\\n    n_layers=n_layers, pretrained_embedding=init_embeddings\\n).to(device)\\ncriterion=nn.BCEWithLogitsLoss().to(device)\\noptimizer=optim.RMSprop(classifier.parameters(), lr=0.001)\\n\\nepochs=5\\ninterval=500\\n\\nfor epoch in range(epochs):\\n    train(classifier, train_loader, criterion, optimizer, device, interval)\\n    test(classifier, test_loader, criterion, device)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**합성곱 신경망**"
      ],
      "metadata": {
        "id": "ZuXlKj6qg7h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "conv=torch.nn.Conv2d(\n",
        "    in_channels,\n",
        "    out_channels,\n",
        "    kernel_size,\n",
        "    stride=1,\n",
        "    padding=0,\n",
        "    dilation=1,\n",
        "    groups=1,\n",
        "    bias=True,\n",
        "    padding_mode='zeros'\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2Xh8DmHXg9nv",
        "outputId": "2beb321a-5892-4d39-d2a6-79ddad780e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nconv=torch.nn.Conv2d(\\n    in_channels,\\n    out_channels,\\n    kernel_size,\\n    stride=1,\\n    padding=0,\\n    dilation=1,\\n    groups=1,\\n    bias=True,\\n    padding_mode='zeros'\\n)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "pool=torch.nn.MaxPool2d(\n",
        "    kernel_size,\n",
        "    stride=None,\n",
        "    padding=0,\n",
        "    dilation=1\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "96vMVHVO9fk2",
        "outputId": "b07e67e5-3a96-4d7f-d347-f92356e3ff30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npool=torch.nn.MaxPool2d(\\n    kernel_size,\\n    stride=None,\\n    padding=0,\\n    dilation=1\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "pool=torch.nn.AvgPool2d(\n",
        "    kernel_size,\n",
        "    stride=None,\n",
        "    padding=0,\n",
        "    count_include_pad=True\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Yz33ASxr9qvT",
        "outputId": "4b7c20e9-b318-41c3-d32b-22d46ebaff1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npool=torch.nn.AvgPool2d(\\n    kernel_size,\\n    stride=None,\\n    padding=0,\\n    count_include_pad=True\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#합성곱 모델\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1=nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.conv2=nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.fc=nn.Linear(32 * 32 * 32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=self.conv1(x)\n",
        "        x=self.conv2(x)\n",
        "        x=torch.flatten(x)\n",
        "        x=self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WCbQBm-0-nIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#합성곱 기반 문장 분류 모델 정의\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_embedding, filter_sizes, max_length, dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding=nn.Embedding.from_pretrained(\n",
        "            torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
        "        )\n",
        "        embedding_dim=self.embedding.weight.shape[1]\n",
        "\n",
        "        conv=[]\n",
        "        for size in filter_sizes:\n",
        "            conv.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(\n",
        "                        in_channels=embedding_dim,\n",
        "                        out_channels=1,\n",
        "                        kernel_size=size\n",
        "                    ),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool1d(kernel_size=max_length-size-1),\n",
        "                )\n",
        "            )\n",
        "        self.conv_filters=nn.ModuleList(conv)\n",
        "\n",
        "        output_size=len(filter_sizes)\n",
        "        self.pre_classifier=nn.Linear(output_size, output_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        self.classifier=nn.Linear(output_size, 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings=self.embedding(inputs)\n",
        "        embeddings=embeddings.permute(0, 2, 1)\n",
        "\n",
        "        conv_outputs=[conv(embeddings) for conv in self.conv_filters]\n",
        "        concat_outputs=torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
        "\n",
        "        logits=self.pre_classifier(concat_outputs)\n",
        "        logits=self.dropout(logits)\n",
        "        logits=self.classifier(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "u0SjgtKd_k-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터세트 불러오기\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "corpus=Korpora.load(\"nsmc\")\n",
        "corpus_df=pd.DataFrame(corpus.test)\n",
        "\n",
        "train=corpus_df.sample(frac=0.9, random_state=42)\n",
        "test=corpus_df.drop(train.index)\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(\"Training Data Size:\", len(train))\n",
        "print(\"Testing Data Size:\", len(test))\n",
        "\n",
        "#데이터 토큰화 및 단어 사전 구축\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter=Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab=special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "tokenizer=Okt()\n",
        "train_tokens=[tokenizer.morphs(review) for review in train.text]\n",
        "test_tokens=[tokenizer.morphs(review) for review in test.text]\n",
        "\n",
        "vocab=build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "token_to_id={token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token={idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))\n",
        "\n",
        "#정수 인코딩 및 패딩\n",
        "import numpy as np\n",
        "\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = list()\n",
        "    for sequence in sequences:\n",
        "        sequence=sequence[:max_length]\n",
        "        pad_length=max_length - len(sequence)\n",
        "        padded_sequence=sequence + [pad_value] * pad_length\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n",
        "\n",
        "unk_id=token_to_id[\"<unk>\"]\n",
        "train_ids=[\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
        "]\n",
        "test_ids=[\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
        "]\n",
        "\n",
        "max_length=32\n",
        "pad_id=token_to_id[\"<pad>\"]\n",
        "train_ids=pad_sequences(train_ids, max_length, pad_id)\n",
        "test_ids=pad_sequences(test_ids, max_length, pad_id)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(test_ids[0])\n",
        "\n",
        "#데이터로더 적용\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "train_ids=torch.tensor(train_ids)\n",
        "test_ids=torch.tensor(test_ids)\n",
        "\n",
        "train_labels=torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels=torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset=TensorDataset(train_ids, train_labels)\n",
        "test_dataset=TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader=DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader=DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "315f997e-a62a-41b4-afd5-e36f3e30a926",
        "id": "jt7DRHurGeWJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n",
            "|       | text                                                                                     |   label |\n",
            "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
            "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
            "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
            "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
            "| 12447 | 잔잔 격동                                                                                |       1 |\n",
            "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
            "Training Data Size: 45000\n",
            "Testing Data Size: 5000\n",
            "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
            "5002\n",
            "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
            " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
            "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#손실 함수와 최적화 함수 정의\n",
        "'''\n",
        "from torch import optim\n",
        "\n",
        "n_vocab=len(token_to_id)\n",
        "hidden_dim=64\n",
        "embedding_dim=128\n",
        "n_layers=2\n",
        "\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "filter_sizes=[3,3,4,4,5,5]\n",
        "classifier=SentenceClassifier(\n",
        "    pretrained_embedding=init_embeddings,\n",
        "    filter_sizes=filter_sizes,\n",
        "    max_length=max_length\n",
        ").to(device)\n",
        "\n",
        "criterion=nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer=optim.Adam(classifier.parameters(), lr=0.001)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "8bti71MGGeWK",
        "outputId": "0750a1ab-ea7a-409d-b561-cc3f31b9c250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom torch import optim\\n\\nn_vocab=len(token_to_id)\\nhidden_dim=64\\nembedding_dim=128\\nn_layers=2\\n\\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\\nfilter_sizes=[3,3,4,4,5,5]\\nclassifier=SentenceClassifier(\\n    pretrained_embedding=init_embeddings,\\n    filter_sizes=filter_sizes,\\n    max_length=max_length\\n).to(device)\\n\\ncriterion=nn.BCEWithLogitsLoss().to(device)\\noptimizer=optim.Adam(classifier.parameters(), lr=0.001)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습 및 테스트\n",
        "'''\n",
        "def train(model, datasets, criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses=list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids=input_ids.to(device)\n",
        "        labels=labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits=model(input_ids)\n",
        "        loss=criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f\"Train Loss {step}: {np.mean(losses)}\")\n",
        "\n",
        "\n",
        "def test(model, datasets, criterion, device):\n",
        "    model.eval()\n",
        "    losses=list()\n",
        "    corrects=list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids=input_ids.to(device)\n",
        "        labels=labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits=model(input_ids)\n",
        "        loss=criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "        yhat=torch.sigmoid(logits)>.5\n",
        "        corrects.extend(\n",
        "            torch.eq(yhat, labels).cpu().tolist()\n",
        "        )\n",
        "\n",
        "    print(f\"Val Loss: {np.mean(losses)}, Val Accuracy: {np.mean(corrects)}\")\n",
        "\n",
        "\n",
        "epochs=5\n",
        "interval=500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "LZqse7hmPK3X",
        "outputId": "7672706e-5a45-4f89-df93-51aa3d290969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef train(model, datasets, criterion, optimizer, device, interval):\\n    model.train()\\n    losses=list()\\n\\n    for step, (input_ids, labels) in enumerate(datasets):\\n        input_ids=input_ids.to(device)\\n        labels=labels.to(device).unsqueeze(1)\\n\\n        logits=model(input_ids)\\n        loss=criterion(logits, labels)\\n        losses.append(loss.item())\\n\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        if step % interval == 0:\\n            print(f\"Train Loss {step}: {np.mean(losses)}\")\\n\\n\\ndef test(model, datasets, criterion, device):\\n    model.eval()\\n    losses=list()\\n    corrects=list()\\n\\n    for step, (input_ids, labels) in enumerate(datasets):\\n        input_ids=input_ids.to(device)\\n        labels=labels.to(device).unsqueeze(1)\\n\\n        logits=model(input_ids)\\n        loss=criterion(logits, labels)\\n        losses.append(loss.item())\\n        yhat=torch.sigmoid(logits)>.5\\n        corrects.extend(\\n            torch.eq(yhat, labels).cpu().tolist()\\n        )\\n\\n    print(f\"Val Loss: {np.mean(losses)}, Val Accuracy: {np.mean(corrects)}\")\\n\\n\\nepochs=5\\ninterval=500\\n\\nfor epoch in range(epochs):\\n    train(classifier, train_loader, criterion, optimizer, device, interval)\\n    test(classifier, test_loader, criterion, device)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}